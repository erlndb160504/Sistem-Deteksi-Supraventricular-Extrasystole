{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13722144,"sourceType":"datasetVersion","datasetId":8730376}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install wfdb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:41:36.532321Z","iopub.execute_input":"2025-12-14T08:41:36.532931Z","iopub.status.idle":"2025-12-14T08:41:41.188148Z","shell.execute_reply.started":"2025-12-14T08:41:36.532906Z","shell.execute_reply":"2025-12-14T08:41:41.187297Z"}},"outputs":[{"name":"stdout","text":"Collecting wfdb\n  Downloading wfdb-4.3.0-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.13.2)\nRequirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2025.10.0)\nRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.7.2)\nRequirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from wfdb) (1.26.4)\nRequirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.2.3)\nRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.32.5)\nRequirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (1.15.3)\nRequirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (0.13.1)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->wfdb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->wfdb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->wfdb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->wfdb) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->wfdb) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->wfdb) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2025.10.5)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\nRequirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.23)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.4->wfdb) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.4->wfdb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.4->wfdb) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.4->wfdb) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.4->wfdb) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.4->wfdb) (2024.2.0)\nDownloading wfdb-4.3.0-py3-none-any.whl (163 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: wfdb\nSuccessfully installed wfdb-4.3.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import subprocess\nimport sys\n\nprint(\"Installing compatible protobuf version...\")\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"protobuf==3.20.0\", \"-q\"])\nprint(\"Done!\")\n\n# Sekarang restart kernel/notebook\nprint(\"\\nKERNEL RESTART REQUIRED!\")\nprint(\"Please restart the kernel now (Jupyter will prompt you)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:41:41.189956Z","iopub.execute_input":"2025-12-14T08:41:41.190188Z","iopub.status.idle":"2025-12-14T08:41:44.989413Z","shell.execute_reply.started":"2025-12-14T08:41:41.190162Z","shell.execute_reply":"2025-12-14T08:41:44.988656Z"}},"outputs":[{"name":"stdout","text":"Installing compatible protobuf version...\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162.1/162.1 kB 5.0 MB/s eta 0:00:00\nDone!\n\nKERNEL RESTART REQUIRED!\nPlease restart the kernel now (Jupyter will prompt you)\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-secret-manager 2.25.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-vision 3.11.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-monitoring 2.28.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-videointelligence 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-api-core 2.28.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.0 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nray 2.51.1 requires protobuf>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-bigtable 2.34.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-discoveryengine 0.13.12 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-aiplatform 1.125.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-speech 2.34.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-logging 3.12.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-trace 1.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-appengine-logging 1.7.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-audit-log 0.4.0 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-functions 1.20.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-resource-manager 1.14.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngoogle-cloud-dataproc 5.21.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.0 which is incompatible.\ngoogleapis-common-protos 1.70.0 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\ngoogle-cloud-datastore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-iam 2.19.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-language 2.17.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-bigquery-connection 1.18.3 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngrpc-google-iam-v1 0.14.2 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-spanner 3.56.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-firestore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import KFold\nfrom scipy.signal import filtfilt, butter\nimport wfdb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:41:44.990754Z","iopub.execute_input":"2025-12-14T08:41:44.991008Z","iopub.status.idle":"2025-12-14T08:42:00.512617Z","shell.execute_reply.started":"2025-12-14T08:41:44.990991Z","shell.execute_reply":"2025-12-14T08:42:00.511882Z"}},"outputs":[{"name":"stderr","text":"2025-12-14 08:41:46.260797: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765701706.431960      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765701706.483530      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# =====================================================\n# 1. IMPROVED PREPROCESSING (Match GUI)\n# =====================================================\n\ndef notch_filter_formula2(signal, f0=50, fs=128, r=0.98):\n    \"\"\"Notch filter 50Hz\"\"\"\n    w0 = 2 * np.pi * f0 / fs\n    c = np.cos(w0)\n    b = np.array([1, -2*c, 1])\n    a = np.array([1, -2*r*c, r*r])\n    return filtfilt(b, a, signal)\n\ndef bandpass_filter(signal, low=0.5, high=50, fs=128, order=4):\n    \"\"\"Bandpass 0.5-50Hz (matches GUI training!)\"\"\"\n    nyq = 0.5 * fs\n    b, a = butter(order, [low/nyq, high/nyq], btype='band')\n    return filtfilt(b, a, signal)\n\ndef normalize_robust(signal):\n    \"\"\"Robust normalization using percentiles 1-99\"\"\"\n    q1, q99 = np.percentile(signal, [1, 99])\n    if q99 - q1 == 0:\n        return np.zeros_like(signal)\n    signal_clipped = np.clip(signal, q1, q99)\n    return (signal_clipped - q1) / (q99 - q1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:42:00.513337Z","iopub.execute_input":"2025-12-14T08:42:00.513780Z","iopub.status.idle":"2025-12-14T08:42:00.519847Z","shell.execute_reply.started":"2025-12-14T08:42:00.513760Z","shell.execute_reply":"2025-12-14T08:42:00.519209Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# =====================================================\n# 2. LOAD SVDB WITH PVC SAMPLES (NEW!)\n# =====================================================\n\ndef load_svdb (sve_symbols={\"A\", \"a\", \"J\", \"S\", \"s\"},\n                       normal_symbols={\"V\", \"v\",\"N\"},\n                       window=512,\n                       fs=128,\n                       mask_width=48,\n                       random_state=42):\n\n    rng = np.random.default_rng(random_state)\n    \n    svdb_records = [\n        '800','801','802','803','804','805','806','807','808','809','811','812','820','821','822','823',\n        '824','825','826','827','828','829','840','841','842','844','846','847','848','849','850','851',\n        '852','853','854','855','856','857','858','859','860','861','862','863','864','865','866','867',\n        '868','869','870','871','872','873','874','875','876','877','878','879','880','881','882','883',\n        '884','885','886','887','888','889','890','891','892','893','894'\n    ]\n\n    X_sve = []\n    y_sve = []\n    X_non_sve = []  \n    y_non_sve = []\n\n    for rec in svdb_records:\n        print(f\"Loading {rec}... \", end=\"\", flush=True)\n        try:\n            sig, fields = wfdb.rdsamp(f\"/kaggle/input/svdbfix/{rec}\")\n            ann = wfdb.rdann(f\"/kaggle/input/svdbfix/{rec}\", \"atr\")\n            ecg = sig[:, 1]\n\n            # Preprocessing (match GUI!)\n            ecg = notch_filter_formula2(ecg, 50, fs, r=0.98)\n            ecg = bandpass_filter(ecg, 0.5, 50, fs, order=4)\n            ecg = normalize_robust(ecg)\n\n            # Create mask for SVE only\n            mask = np.zeros(len(ecg))\n            events = list(zip(ann.sample, ann.symbol))\n\n            # Mark ONLY SVE symbols\n            for samp, sym in events:\n                if sym in sve_symbols:\n                    st = max(0, samp - mask_width)\n                    ed = min(len(ecg), samp + mask_width)\n                    mask[st:ed] = 1\n\n            total = len(ecg) // window\n            for w in range(total):\n                s = w * window\n                e = s + window\n                win_syms = {sym for samp, sym in events if s <= samp < e}\n\n                # Check what symbols are present\n                has_sve = any(sym in sve_symbols for sym in win_syms)\n                has_pvc = any(sym in normal_symbols for sym in win_syms)\n                has_unknown = any(sym not in sve_symbols and sym not in normal_symbols and sym != '' for sym in win_syms)\n\n                # Skip if has unknown symbols\n                if has_unknown:\n                    continue\n\n                win_sig = ecg[s:e]\n                win_mask = mask[s:e]\n\n                # IMPORTANT: If has SVE, go to SVE class\n                if has_sve:\n                    X_sve.append(win_sig)\n                    y_sve.append(win_mask)\n                # Otherwise (PVC or Normal), go to non-SVE class\n                else:\n                    X_non_sve.append(win_sig)\n                    y_non_sve.append(win_mask)  # All zeros for non-SVE\n\n            print(\"✓\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n\n    X_sve = np.array(X_sve).reshape(-1, window, 1)\n    y_sve = np.array(y_sve).reshape(-1, window, 1)\n    X_non_sve = np.array(X_non_sve).reshape(-1, window, 1)\n    y_non_sve = np.array(y_non_sve).reshape(-1, window, 1)\n\n    print(f\"\\nDataset before balancing:\")\n    print(f\"SVE samples: {X_sve.shape[0]}\")\n    print(f\"Non-SVE (PVC+Normal) samples: {X_non_sve.shape[0]}\")\n\n    # Balance: keep same amount of SVE and non-SVE\n    target = min(X_sve.shape[0], X_non_sve.shape[0])\n\n    if X_sve.shape[0] > target:\n        idx = rng.choice(X_sve.shape[0], target, replace=False)\n        X_sve = X_sve[idx]\n        y_sve = y_sve[idx]\n\n    if X_non_sve.shape[0] > target:\n        idx = rng.choice(X_non_sve.shape[0], target, replace=False)\n        X_non_sve = X_non_sve[idx]\n        y_non_sve = y_non_sve[idx]\n\n    # Combine\n    X_all = np.concatenate([X_sve, X_non_sve], 0)\n    y_all = np.concatenate([y_sve, y_non_sve], 0)\n\n    print(f\"\\nDataset after balancing:\")\n    print(f\"SVE: {X_sve.shape[0]}\")\n    print(f\"Non-SVE: {X_non_sve.shape[0]}\")\n    print(f\"Total: {X_all.shape}\")\n\n    # Shuffle\n    idx = rng.permutation(X_all.shape[0])\n    return X_all[idx], y_all[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:42:00.520667Z","iopub.execute_input":"2025-12-14T08:42:00.520865Z","iopub.status.idle":"2025-12-14T08:42:00.536970Z","shell.execute_reply.started":"2025-12-14T08:42:00.520842Z","shell.execute_reply":"2025-12-14T08:42:00.536403Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# =====================================================\n# 3. LOSS FUNCTIONS & METRICS\n# =====================================================\n\ndef dice_coefficient(y_true, y_pred, smooth=1e-6):\n    \"\"\"Dice loss\"\"\"\n    y_true_f = tf.reshape(y_true, [-1])\n    y_pred_f = tf.reshape(y_pred, [-1])\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) +\n                                           tf.reduce_sum(y_pred_f) + smooth)\n\ndef weighted_focal_tversky_loss(y_true, y_pred,\n                                alpha=0.6, beta=0.4, gamma=2.5, smooth=1e-6):\n    \"\"\"Weighted focal tversky loss\"\"\"\n    y_true_f = tf.reshape(y_true, [-1])\n    y_pred_f = tf.reshape(y_pred, [-1])\n    tp = tf.reduce_sum(y_true_f * y_pred_f)\n    fp = tf.reduce_sum((1 - y_true_f) * y_pred_f)\n    fn = tf.reduce_sum(y_true_f * (1 - y_pred_f))\n    tversky = (tp + smooth) / (tp + alpha*fn + beta*fp + smooth)\n    return tf.pow((1 - tversky), gamma)\n\ndef norm_accuracy(y_true, y_pred, threshold=0.5, smooth=1e-6):\n    \"\"\"Normalized accuracy\"\"\"\n    y_true_f = tf.reshape(y_true, [-1])\n    y_pred_f = tf.cast(tf.reshape(y_pred, [-1]) > threshold, tf.float32)\n    diff = y_pred_f - y_true_f\n    norm = tf.norm(diff, ord=2)\n    denom = tf.sqrt(tf.cast(tf.size(y_true_f), tf.float32))\n    return 1.0 - (norm / (denom + smooth))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:42:00.537646Z","iopub.execute_input":"2025-12-14T08:42:00.538192Z","iopub.status.idle":"2025-12-14T08:42:00.553327Z","shell.execute_reply.started":"2025-12-14T08:42:00.538169Z","shell.execute_reply":"2025-12-14T08:42:00.552809Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def compare_preprocessing_multiple_records(record_list=['800', '801', '802'], \n                                          start_sample=0, \n                                          window_size=1024,\n                                          fs=128):\n    \"\"\"\n    Bandingkan preprocessing untuk beberapa record sekaligus\n    \n    Args:\n        record_list: list nama record SVDB\n        start_sample: sample awal\n        window_size: ukuran window\n        fs: sampling frequency\n    \"\"\"\n    \n    n_records = len(record_list)\n    fig, axes = plt.subplots(n_records, 4, figsize=(20, 4*n_records))\n    \n    if n_records == 1:\n        axes = axes.reshape(1, -1)\n    \n    fig.suptitle(f'Preprocessing Comparison - {n_records} Records\\n'\n                 f'Window: {window_size} samples ({window_size/fs:.1f}s)', \n                 fontsize=16, fontweight='bold', y=0.995)\n    \n    all_results = {}\n    \n    for idx, record_name in enumerate(record_list):\n        print(f\"\\nProcessing record {record_name}...\")\n        \n        try:\n            # Load data\n            sig, fields = wfdb.rdsamp(f\"/kaggle/input/svdbfix/{record_name}\")\n            ecg_raw = sig[:, 1]\n            \n            # Extract window\n            end_sample = min(start_sample + window_size, len(ecg_raw))\n            ecg_segment = ecg_raw[start_sample:end_sample]\n            \n            # Process stages\n            stage1_raw = ecg_segment.copy()\n            stage2_notch = notch_filter_formula2(stage1_raw, f0=50, fs=fs, r=0.98)\n            stage3_bandpass = bandpass_filter(stage2_notch, low=0.5, high=50, fs=fs, order=4)\n            stage4_normalized = normalize_robust(stage3_bandpass)\n            \n            # Time axis\n            time = np.arange(len(ecg_segment)) / fs\n            \n            # Plot all 4 stages\n            stages = [\n                (stage1_raw, 'Raw Signal', 'blue'),\n                (stage2_notch, 'After Notch', 'green'),\n                (stage3_bandpass, 'After Bandpass', 'orange'),\n                (stage4_normalized, 'Normalized', 'red')\n            ]\n            \n            for col, (data, title, color) in enumerate(stages):\n                ax = axes[idx, col]\n                ax.plot(time, data, color=color, linewidth=0.7, alpha=0.8)\n                \n                if idx == 0:\n                    ax.set_title(title, fontweight='bold', fontsize=11)\n                \n                if col == 0:\n                    ax.set_ylabel(f'Record {record_name}\\n(mV)', fontsize=9, fontweight='bold')\n                elif col == 3:\n                    ax.set_ylabel(f'Record {record_name}\\n(norm)', fontsize=9, fontweight='bold')\n                \n                if idx == n_records - 1:\n                    ax.set_xlabel('Time (s)', fontsize=9)\n                \n                ax.grid(True, alpha=0.3, linewidth=0.5)\n                ax.set_xlim([time[0], time[-1]])\n                \n                # Add stats box\n                stats = f'μ={np.mean(data):.2f}\\nσ={np.std(data):.2f}'\n                ax.text(0.02, 0.98, stats, transform=ax.transAxes,\n                       fontsize=8, verticalalignment='top',\n                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n            \n            # Store results\n            all_results[record_name] = {\n                'raw': stage1_raw,\n                'notch': stage2_notch,\n                'bandpass': stage3_bandpass,\n                'normalized': stage4_normalized\n            }\n            \n            print(f\"  ✓ {record_name} processed\")\n            \n        except Exception as e:\n            print(f\"  ❌ Error processing {record_name}: {e}\")\n            continue\n    \n    plt.tight_layout()\n    plt.savefig(f'preprocessing_comparison_{len(record_list)}_records.png', \n                dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"\\n✓ Saved: preprocessing_comparison_{len(record_list)}_records.png\")\n    \n    return all_results\n\n\ndef show_frequency_spectrum(record_name='800', start_sample=0, window_size=2048, fs=128):\n    \"\"\"\n    Tampilkan spektrum frekuensi sebelum dan sesudah filtering\n    \"\"\"\n    \n    print(f\"Analyzing frequency spectrum for record {record_name}...\")\n    \n    # Load data\n    sig, fields = wfdb.rdsamp(f\"/kaggle/input/svdbfix/{record_name}\")\n    ecg_raw = sig[:, 1]\n    \n    # Extract window\n    end_sample = min(start_sample + window_size, len(ecg_raw))\n    ecg_segment = ecg_raw[start_sample:end_sample]\n    \n    # Process\n    stage1_raw = ecg_segment.copy()\n    stage2_notch = notch_filter_formula2(stage1_raw, f0=50, fs=fs, r=0.98)\n    stage3_bandpass = bandpass_filter(stage2_notch, low=0.5, high=50, fs=fs, order=4)\n    stage4_normalized = normalize_robust(stage3_bandpass)\n    \n    # Compute FFT\n    def compute_fft(signal, fs):\n        n = len(signal)\n        fft_vals = np.fft.rfft(signal)\n        fft_freq = np.fft.rfftfreq(n, 1/fs)\n        fft_power = np.abs(fft_vals) ** 2\n        return fft_freq, fft_power\n    \n    freq1, power1 = compute_fft(stage1_raw, fs)\n    freq2, power2 = compute_fft(stage2_notch, fs)\n    freq3, power3 = compute_fft(stage3_bandpass, fs)\n    \n    # Plot\n    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n    \n    # Time domain - before\n    axes[0, 0].plot(np.arange(len(stage1_raw))/fs, stage1_raw, 'b-', linewidth=0.7, alpha=0.8)\n    axes[0, 0].set_title('Time Domain - Raw Signal', fontweight='bold', fontsize=12)\n    axes[0, 0].set_xlabel('Time (s)')\n    axes[0, 0].set_ylabel('Amplitude (mV)')\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # Frequency domain - before\n    axes[0, 1].semilogy(freq1, power1, 'b-', linewidth=1, alpha=0.8)\n    axes[0, 1].axvline(50, color='red', linestyle='--', linewidth=2, label='50Hz (Power Line)')\n    axes[0, 1].set_title('Frequency Domain - Raw Signal', fontweight='bold', fontsize=12)\n    axes[0, 1].set_xlabel('Frequency (Hz)')\n    axes[0, 1].set_ylabel('Power (log scale)')\n    axes[0, 1].set_xlim([0, fs/2])\n    axes[0, 1].legend()\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # Time domain - after filtering\n    axes[1, 0].plot(np.arange(len(stage3_bandpass))/fs, stage3_bandpass, 'orange', linewidth=0.7, alpha=0.8)\n    axes[1, 0].set_title('Time Domain - After Filtering', fontweight='bold', fontsize=12)\n    axes[1, 0].set_xlabel('Time (s)')\n    axes[1, 0].set_ylabel('Amplitude (mV)')\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    # Frequency domain - after filtering\n    axes[1, 1].semilogy(freq3, power3, 'orange', linewidth=1, alpha=0.8)\n    axes[1, 1].axvline(0.5, color='green', linestyle='--', linewidth=1.5, label='0.5Hz (Low cutoff)')\n    axes[1, 1].axvline(50, color='red', linestyle='--', linewidth=1.5, label='50Hz (High cutoff)')\n    axes[1, 1].set_title('Frequency Domain - After Filtering', fontweight='bold', fontsize=12)\n    axes[1, 1].set_xlabel('Frequency (Hz)')\n    axes[1, 1].set_ylabel('Power (log scale)')\n    axes[1, 1].set_xlim([0, fs/2])\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n    \n    plt.suptitle(f'Frequency Analysis - Record {record_name}', fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(f'frequency_analysis_{record_name}.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"✓ Saved: frequency_analysis_{record_name}.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:42:00.555482Z","iopub.execute_input":"2025-12-14T08:42:00.555973Z","iopub.status.idle":"2025-12-14T08:42:00.575850Z","shell.execute_reply.started":"2025-12-14T08:42:00.555957Z","shell.execute_reply":"2025-12-14T08:42:00.575264Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# =====================================================\n# USAGE EXAMPLES\n# =====================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"PREPROCESSING VISUALIZATION TOOLS\")\nprint(\"=\"*70)\n\nprint(\"\\n1. Compare multiple records:\")\nprint(\"   results = compare_preprocessing_multiple_records(['800', '801', '802'])\")\n\nprint(\"\\n2. Analyze frequency spectrum:\")\nprint(\"   show_frequency_spectrum('800', start_sample=0, window_size=2048)\")\n\nprint(\"\\n3. Custom comparison:\")\nprint(\"   results = compare_preprocessing_multiple_records(\")\nprint(\"       record_list=['800', '825', '850'],\")\nprint(\"       start_sample=1000,\")\nprint(\"       window_size=512)\")\n\nprint(\"\\n\" + \"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:42:00.576502Z","iopub.execute_input":"2025-12-14T08:42:00.576837Z","iopub.status.idle":"2025-12-14T08:42:00.593158Z","shell.execute_reply.started":"2025-12-14T08:42:00.576812Z","shell.execute_reply":"2025-12-14T08:42:00.592553Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nPREPROCESSING VISUALIZATION TOOLS\n======================================================================\n\n1. Compare multiple records:\n   results = compare_preprocessing_multiple_records(['800', '801', '802'])\n\n2. Analyze frequency spectrum:\n   show_frequency_spectrum('800', start_sample=0, window_size=2048)\n\n3. Custom comparison:\n   results = compare_preprocessing_multiple_records(\n       record_list=['800', '825', '850'],\n       start_sample=1000,\n       window_size=512)\n\n======================================================================\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# =====================================================\n# 4. BUILD UNET 1D MODEL\n# =====================================================\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models\n\ndef build_unet_1d(input_length=512, dr=0.15):\n    \"\"\"UNet 1D architecture\"\"\"\n    inputs = layers.Input((input_length, 1))\n    \n    # Encoder 1\n    c1 = layers.Conv1D(32, 3, padding='same', activation='relu')(inputs)\n    c1 = layers.BatchNormalization()(c1)\n    c1 = layers.Dropout(dr)(c1)\n    c1 = layers.Conv1D(32, 3, padding='same', activation='relu')(c1)\n    c1 = layers.BatchNormalization()(c1)\n    p1 = layers.MaxPooling1D(2)(c1)\n    \n    # Encoder 2\n    c2 = layers.Conv1D(64, 3, padding='same', activation='relu')(p1)\n    c2 = layers.BatchNormalization()(c2)\n    c2 = layers.Dropout(dr)(c2)\n    c2 = layers.Conv1D(64, 3, padding='same', activation='relu')(c2)\n    c2 = layers.BatchNormalization()(c2)\n    p2 = layers.MaxPooling1D(2)(c2)\n    \n    # Encoder 3\n    c3 = layers.Conv1D(128, 3, padding='same', activation='relu')(p2)\n    c3 = layers.BatchNormalization()(c3)\n    c3 = layers.Dropout(dr)(c3)\n    c3 = layers.Conv1D(128, 3, padding='same', activation='relu')(c3)\n    c3 = layers.BatchNormalization()(c3)\n    p3 = layers.MaxPooling1D(2)(c3)\n    \n    # Encoder 4\n    c4 = layers.Conv1D(256, 3, padding='same', activation='relu')(p3)\n    c4 = layers.BatchNormalization()(c4)\n    c4 = layers.Dropout(dr)(c4)\n    c4 = layers.Conv1D(256, 3, padding='same', activation='relu')(c4)\n    c4 = layers.BatchNormalization()(c4)\n    p4 = layers.MaxPooling1D(2)(c4)\n    \n    # Bottleneck\n    bn = layers.Conv1D(512, 3, padding='same', activation='relu')(p4)\n    bn = layers.BatchNormalization()(bn)\n    bn = layers.Dropout(dr)(bn)\n    bn = layers.Conv1D(512, 3, padding='same', activation='relu')(bn)\n    bn = layers.BatchNormalization()(bn)\n    \n    # Decoder 1\n    u4 = layers.UpSampling1D(2)(bn)\n    u4 = layers.Concatenate()([u4, c4])\n    d4 = layers.Conv1D(256, 3, padding='same', activation='relu')(u4)\n    d4 = layers.BatchNormalization()(d4)\n    d4 = layers.Dropout(dr)(d4)\n    d4 = layers.Conv1D(256, 3, padding='same', activation='relu')(d4)\n    d4 = layers.BatchNormalization()(d4)\n    \n    # Decoder 2\n    u3 = layers.UpSampling1D(2)(d4)\n    u3 = layers.Concatenate()([u3, c3])\n    d3 = layers.Conv1D(128, 3, padding='same', activation='relu')(u3)\n    d3 = layers.BatchNormalization()(d3)\n    d3 = layers.Dropout(dr)(d3)\n    d3 = layers.Conv1D(128, 3, padding='same', activation='relu')(d3)\n    d3 = layers.BatchNormalization()(d3)\n    \n    # Decoder 3\n    u2 = layers.UpSampling1D(2)(d3)\n    u2 = layers.Concatenate()([u2, c2])\n    d2 = layers.Conv1D(64, 3, padding='same', activation='relu')(u2)\n    d2 = layers.BatchNormalization()(d2)\n    d2 = layers.Dropout(dr)(d2)\n    d2 = layers.Conv1D(64, 3, padding='same', activation='relu')(d2)\n    d2 = layers.BatchNormalization()(d2)\n    \n    # Decoder 4\n    u1 = layers.UpSampling1D(2)(d2)\n    u1 = layers.Concatenate()([u1, c1])\n    d1 = layers.Conv1D(32, 3, padding='same', activation='relu')(u1)\n    d1 = layers.BatchNormalization()(d1)\n    d1 = layers.Dropout(dr)(d1)\n    d1 = layers.Conv1D(32, 3, padding='same', activation='relu')(d1)\n    d1 = layers.BatchNormalization()(d1)\n    \n    # Output\n    outputs = layers.Conv1D(1, 1, activation='sigmoid')(d1)\n    \n    return models.Model(inputs, outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:42:00.593800Z","iopub.execute_input":"2025-12-14T08:42:00.594108Z","iopub.status.idle":"2025-12-14T08:42:00.663833Z","shell.execute_reply.started":"2025-12-14T08:42:00.594090Z","shell.execute_reply":"2025-12-14T08:42:00.663283Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# =====================================================\n# 5. TRAINING (50-FOLD CV)\n# =====================================================\n\nprint(\"=\"*70)\nprint(\"LOADING SVDB DATASET WITH PVC SAMPLES\")\nprint(\"=\"*70)\nX, y = load_svdb()\nprint(f\"\\nDataset shape: {X.shape}, {y.shape}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"STARTING 50-FOLD CROSS VALIDATION WITH PVC TRAINING\")\nprint(\"=\"*70)\n\nkfold = KFold(n_splits=50, shuffle=True, random_state=42)\nall_loss = []\nall_dice = []\nall_norm = []\nall_prec = []\nall_rec = []\nall_f1 = []\n\nfold = 1\nfor train_idx, val_idx in kfold.split(X):\n    print(f\"\\nFOLD {fold}/50 | Train {len(train_idx)} | Val {len(val_idx)}\")\n\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    # Build model\n    model = build_unet_1d(512, dr=0.15)\n\n    # Compile\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n        loss='binary_crossentropy',\n        metrics=[\n            dice_coefficient,\n            norm_accuracy,\n            tf.keras.metrics.Precision(name='precision'),\n            tf.keras.metrics.Recall(name='recall')\n        ]\n    )\n\n    # Train\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=100,\n        batch_size=32,\n        verbose=1,\n        callbacks=[\n            tf.keras.callbacks.EarlyStopping(\n                monitor='val_recall',\n                patience=12,\n                restore_best_weights=True,\n                mode='max'),\n            tf.keras.callbacks.ReduceLROnPlateau(\n                monitor='val_loss',\n                factor=0.5,\n                patience=5,\n                min_lr=1e-6)\n        ]\n    )\n\n    # Evaluate\n    results = model.evaluate(X_val, y_val, verbose=0)\n    loss, dice, norm_acc, prec, rec = results\n    f1 = 2 * (prec * rec) / (prec + rec + 1e-6)\n\n    print(f\"  Results: Dice={dice*100:.1f}% | NormAcc={norm_acc*100:.1f}% | \"\n          f\"P={prec*100:.1f}% | R={rec*100:.1f}% | F1={f1*100:.1f}%\")\n\n    all_loss.append(loss)\n    all_dice.append(dice)\n    all_norm.append(norm_acc)\n    all_prec.append(prec)\n    all_rec.append(rec)\n    all_f1.append(f1)\n\n    # Save model\n    model.save(f\"unet_fold_{fold}_with_pvc.h5\")\n    print(f\"  Saved: unet_fold_{fold}_with_pvc.h5\")\n    \n    fold += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:42:00.664515Z","iopub.execute_input":"2025-12-14T08:42:00.664745Z","iopub.status.idle":"2025-12-14T08:42:48.673453Z","shell.execute_reply.started":"2025-12-14T08:42:00.664725Z","shell.execute_reply":"2025-12-14T08:42:48.672463Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nLOADING SVDB DATASET WITH PVC SAMPLES\n======================================================================\nLoading 800... ✓\nLoading 801... ✓\nLoading 802... ✓\nLoading 803... ✓\nLoading 804... ✓\nLoading 805... ✓\nLoading 806... ✓\nLoading 807... ✓\nLoading 808... ✓\nLoading 809... ✓\nLoading 811... ✓\nLoading 812... ✓\nLoading 820... ✓\nLoading 821... ✓\nLoading 822... ✓\nLoading 823... ✓\nLoading 824... ✓\nLoading 825... ✓\nLoading 826... ✓\nLoading 827... ✓\nLoading 828... ✓\nLoading 829... ✓\nLoading 840... ✓\nLoading 841... ✓\nLoading 842... ✓\nLoading 844... ✓\nLoading 846... ✓\nLoading 847... ✓\nLoading 848... ✓\nLoading 849... ✓\nLoading 850... ✓\nLoading 851... ✓\nLoading 852... ✓\nLoading 853... ✓\nLoading 854... ✓\nLoading 855... ✓\nLoading 856... ✓\nLoading 857... ✓\nLoading 858... ✓\nLoading 859... ✓\nLoading 860... ✓\nLoading 861... ✓\nLoading 862... ✓\nLoading 863... ✓\nLoading 864... ✓\nLoading 865... ✓\nLoading 866... ✓\nLoading 867... ✓\nLoading 868... ✓\nLoading 869... ✓\nLoading 870... ✓\nLoading 871... ✓\nLoading 872... ✓\nLoading 873... ✓\nLoading 874... ✓\nLoading 875... ✓\nLoading 876... ✓\nLoading 877... ✓\nLoading 878... ✓\nLoading 879... ✓\nLoading 880... ✓\nLoading 881... ✓\nLoading 882... ✓\nLoading 883... ✓\nLoading 884... ✓\nLoading 885... ✓\nLoading 886... ✓\nLoading 887... ✓\nLoading 888... ✓\nLoading 889... ✓\nLoading 890... ✓\nLoading 891... ✓\nLoading 892... ✓\nLoading 893... ✓\nLoading 894... ✓\n\nDataset before balancing:\nSVE samples: 6443\nNon-SVE (PVC+Normal) samples: 25097\n\nDataset after balancing:\nSVE: 6443\nNon-SVE: 6443\nTotal: (12886, 512, 1)\n\nDataset shape: (12886, 512, 1), (12886, 512, 1)\n\n======================================================================\nSTARTING 50-FOLD CROSS VALIDATION WITH PVC TRAINING\n======================================================================\n\nFOLD 1/50 | Train 12628 | Val 258\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1765701732.636659      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1765701752.039770     117 service.cc:148] XLA service 0x7f8fb8002d60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1765701752.040397     117 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1765701753.859192     117 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  7/395\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - dice_coefficient: 0.1992 - loss: 0.8473 - norm_accuracy: 0.3121 - precision: 0.1270 - recall: 0.4744  ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1765701768.187002     117 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2580470331.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[1;32m    219\u001b[0m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mhas_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       return gen_optional_ops.optional_has_value(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\u001b[0m in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    173\u001b[0m         _ctx, \"OptionalHasValue\", name, optional)\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"# =====================================================\n# 6. RESULTS SUMMARY\n# =====================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING COMPLETE - FINAL RESULTS\")\nprint(\"=\"*70)\nprint(f\"\\nAverage Dice Coefficient: {np.mean(all_dice)*100:.2f}% ± {np.std(all_dice)*100:.2f}%\")\nprint(f\"Average Norm Accuracy:    {np.mean(all_norm)*100:.2f}% ± {np.std(all_norm)*100:.2f}%\")\nprint(f\"Average Precision:        {np.mean(all_prec)*100:.2f}% ± {np.std(all_prec)*100:.2f}%\")\nprint(f\"Average Recall:           {np.mean(all_rec)*100:.2f}% ± {np.std(all_rec)*100:.2f}%\")\nprint(f\"Average F1 Score:         {np.mean(all_f1)*100:.2f}% ± {np.std(all_f1)*100:.2f}%\")\n\n# Select best model\nbest_fold = np.argmax(all_f1) + 1\nprint(f\"\\n✅ Best model: unet_fold_{best_fold}_with_pvc.h5\")\nprint(f\"   F1 Score: {all_f1[best_fold-1]*100:.2f}%\")\n\nprint(\"\\n✅ TRAINING DONE!\")\nprint(\"Next: Load best model into GUI and test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:42:48.674139Z","iopub.status.idle":"2025-12-14T08:42:48.674390Z","shell.execute_reply.started":"2025-12-14T08:42:48.674259Z","shell.execute_reply":"2025-12-14T08:42:48.674269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================\n# ENSEMBLE - SIMPLE VERSION\n# =====================================================\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\n\nprint(\"\\nLoading 50 models...\")\n\nmodels_ensemble = []\nfold_dice_list = []\n\nfor fold_no in range(1, len(all_dice) + 1):\n    model_path = f\"unet_fold_{fold_no}_with_pvc.h5\"\n    \n    try:\n        model = load_model(model_path, compile=False)\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n            loss=lambda y_true, y_pred: dice_bce_hybrid_loss(y_true, y_pred, alpha=0.5),\n            metrics=[dice_coefficient, norm_accuracy]\n        )\n        models_ensemble.append(model)\n        fold_dice_list.append(float(all_dice[fold_no - 1]))\n        \n        if fold_no % 10 == 0:\n            print(f\"  Fold {fold_no}: OK\")\n    except:\n        pass\n\nprint(f\"Loaded: {len(models_ensemble)}/50\\n\")\n\n# =====================================================\n# ENSEMBLE FUNCTIONS\n# =====================================================\n\ndef ensemble_predict_average(X):\n    preds = [model.predict(X, verbose=0) for model in models_ensemble]\n    return np.mean(preds, axis=0)\n\ndef ensemble_predict_weighted(X):\n    preds = [model.predict(X, verbose=0) for model in models_ensemble]\n    weights = np.array(fold_dice_list) / np.sum(fold_dice_list)\n    result = np.zeros_like(preds[0])\n    for pred, w in zip(preds, weights):\n        result += pred * w\n    return result\n\ndef ensemble_predict_median(X):\n    preds = [model.predict(X, verbose=0) for model in models_ensemble]\n    return np.median(preds, axis=0)\n\n# =====================================================\n# TEST ENSEMBLE\n# =====================================================\n\nprint(\"Testing ensemble...\\n\")\n\n# Test data\ntest_size = min(200, X_val.shape[0])\nX_test = X_val[:test_size]\ny_test = tf.cast(y_val[:test_size], tf.float32)\n\n# Average\npred_avg = tf.cast(ensemble_predict_average(X_test), tf.float32)\ndice_avg = dice_coefficient(y_test, pred_avg).numpy()\n\n# Weighted\npred_weighted = tf.cast(ensemble_predict_weighted(X_test), tf.float32)\ndice_weighted = dice_coefficient(y_test, pred_weighted).numpy()\n\n# Median\npred_median = tf.cast(ensemble_predict_median(X_test), tf.float32)\ndice_median = dice_coefficient(y_test, pred_median).numpy()\n\n# Results\navg_ind = np.mean(all_dice)\n\nprint(f\"Individual Avg: {avg_ind*100:.2f}%\")\nprint(f\"Average Ensemble: {dice_avg*100:.2f}%\")\nprint(f\"Weighted Ensemble: {dice_weighted*100:.2f}%\")\nprint(f\"Median Ensemble: {dice_median*100:.2f}%\")\n\n# Best method\nbest = max(dice_avg, dice_weighted, dice_median)\nif best == dice_weighted:\n    best_method = 'weighted'\nelif best == dice_avg:\n    best_method = 'average'\nelse:\n    best_method = 'median'\n\nprint(f\"\\nBest: {best_method.upper()} ({best*100:.2f}%)\")\n\n# =====================================================\n# INFERENCE FUNCTION\n# =====================================================\n\ndef predict_with_ensemble(ecg_512):\n    \"\"\"Predict dengan best method\"\"\"\n    if ecg_512.ndim == 1:\n        ecg_512 = ecg_512.reshape(-1, 1)\n    X = ecg_512.reshape(1, 512, 1)\n    \n    if best_method == 'weighted':\n        return ensemble_predict_weighted(X).squeeze()\n    elif best_method == 'median':\n        return ensemble_predict_median(X).squeeze()\n    else:\n        return ensemble_predict_average(X).squeeze()\n\nprint(\"\\nOK! Use: predict_with_ensemble(X_test[0])\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:42:48.675557Z","iopub.status.idle":"2025-12-14T08:42:48.675857Z","shell.execute_reply.started":"2025-12-14T08:42:48.675732Z","shell.execute_reply":"2025-12-14T08:42:48.675746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================\n# CREATE FINAL MODEL\n# =====================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CREATING FINAL MODEL\")\nprint(\"=\"*60)\n\n# Get ensemble predictions\nprint(\"\\nGenerating pseudo-labels dari ensemble...\")\ny_ensemble = np.mean([model.predict(X, verbose=0) for model in models_ensemble], axis=0)\nprint(f\"✓ Shape: {y_ensemble.shape}\")\n\n# Build model\nprint(\"\\nBuilding UNet model...\")\nfinal_model = build_unet_1d(512)\n\nfinal_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n    loss='binary_crossentropy',\n    metrics=[dice_coefficient, norm_accuracy]\n)\n\n# Train\nprint(\"\\nTraining...\")\nhistory = final_model.fit(\n    X, y_ensemble,\n    epochs=100,\n    batch_size=128,\n    shuffle=True,\n    verbose=1\n)\n\n# Save\nfinal_model.save(\"unet_svdb_FINAL.h5\")\nprint(\"\\n✓ Saved: unet_svdb_FINAL.h5\")\n\n# Test\nprint(\"\\n\" + \"=\"*60)\nprint(\"TEST\")\nprint(\"=\"*60)\n\ntest_size = min(200, X_val.shape[0])\nX_test = X_val[:test_size]\ny_test = tf.cast(y_val[:test_size], tf.float32)\n\npred = tf.cast(final_model.predict(X_test, verbose=0), tf.float32)\ndice = dice_coefficient(y_test, pred).numpy()\n\nprint(f\"\\nFinal Model Dice: {dice*100:.2f}%\")\nprint(f\"✓ READY FOR INFERENCE!\")\n\n# Function\ndef predict_final_model(ecg_512):\n    if ecg_512.ndim == 1:\n        ecg_512 = ecg_512.reshape(-1, 1)\n    X = ecg_512.reshape(1, 512, 1)\n    return final_model.predict(X, verbose=0).squeeze()\n\nprint(\"\\nUsage: pred = predict_final_model(X_test[0])\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:42:48.676958Z","iopub.status.idle":"2025-12-14T08:42:48.677210Z","shell.execute_reply.started":"2025-12-14T08:42:48.677083Z","shell.execute_reply":"2025-12-14T08:42:48.677106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================\n# EVALUATE FINAL MODEL - Run AFTER training complete\n# =====================================================\n\nimport tensorflow as tf\nimport numpy as np\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"LOADING TRAINED MODEL\")\nprint(\"=\"*60)\n\n# Load the model that was trained\nfinal_model = tf.keras.models.load_model(\"unet_svdb_FINAL.h5\")\nprint(\"✓ Model loaded: unet_svdb_FINAL.h5\")\n\n# =====================================================\n# PREPARE TEST DATA\n# =====================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"PREPARING TEST DATA\")\nprint(\"=\"*60)\n\ntest_size = min(200, X_val.shape[0])\nX_test = X_val[:test_size]\ny_test = tf.cast(y_val[:test_size], tf.float32)\n\nprint(f\"✓ Test set size: {test_size} samples\")\nprint(f\"✓ X_test shape: {X_test.shape}\")\nprint(f\"✓ y_test shape: {y_test.shape}\")\n\n# =====================================================\n# GET PREDICTIONS\n# =====================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"RUNNING INFERENCE\")\nprint(\"=\"*60)\n\n# Get probability predictions\npred_probs = final_model.predict(X_test, verbose=0)\npred_probs = tf.cast(pred_probs, tf.float32)\nprint(\"✓ Got probability predictions\")\n\n# Convert to binary (threshold 0.5)\npred_binary = tf.cast(pred_probs > 0.5, tf.float32)\nprint(\"✓ Converted to binary predictions (threshold=0.5)\")\n\n# =====================================================\n# FLATTEN FOR METRICS\n# =====================================================\n\ny_test_flat = tf.reshape(y_test, [-1])\npred_binary_flat = tf.reshape(pred_binary, [-1])\npred_probs_flat = tf.reshape(pred_probs, [-1])\n\nprint(f\"\\n✓ Flattened for metric calculation\")\nprint(f\"  Total predictions: {len(pred_binary_flat):,} samples\")\n\n# =====================================================\n# CALCULATE CONFUSION MATRIX\n# =====================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CALCULATING METRICS\")\nprint(\"=\"*60)\n\ntp = tf.reduce_sum(y_test_flat * pred_binary_flat)\nfp = tf.reduce_sum((1 - y_test_flat) * pred_binary_flat)\nfn = tf.reduce_sum(y_test_flat * (1 - pred_binary_flat))\ntn = tf.reduce_sum((1 - y_test_flat) * (1 - pred_binary_flat))\n\nprint(f\"\\nConfusion Matrix:\")\nprint(f\"  TP (True Positive):   {int(tp.numpy()):,}\")\nprint(f\"  FP (False Positive):  {int(fp.numpy()):,}\")\nprint(f\"  TN (True Negative):   {int(tn.numpy()):,}\")\nprint(f\"  FN (False Negative):  {int(fn.numpy()):,}\")\n\n# =====================================================\n# CALCULATE METRICS\n# =====================================================\n\n# Precision\nprecision = tp / (tp + fp + 1e-7)\n\n# Recall (Sensitivity)\nrecall = tp / (tp + fn + 1e-7)\n\n# F1 Score\nf1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n\n# Accuracy\naccuracy = (tp + tn) / (tp + fp + fn + tn + 1e-7)\n\n# Specificity\nspecificity = tn / (tn + fp + 1e-7)\n\n# Dice Coefficient\ndice = dice_coefficient(y_test, pred_probs).numpy()\n\n# Norm Accuracy\nnorm_acc = norm_accuracy(y_test, pred_probs).numpy()\n\n# =====================================================\n# PRINT ALL RESULTS\n# =====================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL MODEL EVALUATION RESULTS\")\nprint(\"=\"*60)\n\nprint(f\"\\nMetrics on validation set ({test_size} samples):\")\nprint(f\"{'='*60}\")\nprint(f\"Precision:        {precision.numpy()*100:6.2f}%\")\nprint(f\"Recall:           {recall.numpy()*100:6.2f}%\")\nprint(f\"Specificity:      {specificity.numpy()*100:6.2f}%\")\nprint(f\"F1 Score:         {f1.numpy()*100:6.2f}%\")\nprint(f\"Accuracy:         {accuracy.numpy()*100:6.2f}%\")\nprint(f\"Dice Coefficient: {dice*100:6.2f}%\")\nprint(f\"Norm Accuracy:    {norm_acc*100:6.2f}%\")\nprint(f\"{'='*60}\")\n\n# =====================================================\n# CONFUSION MATRIX DETAILS\n# =====================================================\n\nprint(f\"\\nConfusion Matrix Details:\")\nprint(f\"{'='*60}\")\nprint(f\"True Positives:   {int(tp.numpy()):,}\")\nprint(f\"False Positives:  {int(fp.numpy()):,}\")\nprint(f\"True Negatives:   {int(tn.numpy()):,}\")\nprint(f\"False Negatives:  {int(fn.numpy()):,}\")\nprint(f\"{'='*60}\")\n\n# =====================================================\n# QUALITY ASSESSMENT\n# =====================================================\n\nprint(f\"\\n\" + \"=\"*60)\nprint(\"MODEL QUALITY ASSESSMENT\")\nprint(\"=\"*60)\n\nscore_list = []\n\nif recall.numpy() > 0.85:\n    print(f\"  ✅ Recall EXCELLENT ({recall.numpy()*100:.2f}%)\")\n    score_list.append(\"excellent\")\nelif recall.numpy() > 0.80:\n    print(f\"  ✅ Recall GOOD ({recall.numpy()*100:.2f}%)\")\n    score_list.append(\"good\")\nelse:\n    print(f\"  ⚠️  Recall OK ({recall.numpy()*100:.2f}%)\")\n    score_list.append(\"ok\")\n\nif f1.numpy() > 0.83:\n    print(f\"  ✅ F1 EXCELLENT ({f1.numpy()*100:.2f}%)\")\n    score_list.append(\"excellent\")\nelif f1.numpy() > 0.80:\n    print(f\"  ✅ F1 GOOD ({f1.numpy()*100:.2f}%)\")\n    score_list.append(\"good\")\nelse:\n    print(f\"  ⚠️  F1 OK ({f1.numpy()*100:.2f}%)\")\n    score_list.append(\"ok\")\n\nif precision.numpy() > 0.80:\n    print(f\"  ✅ Precision GOOD ({precision.numpy()*100:.2f}%)\")\n    score_list.append(\"good\")\nelif precision.numpy() > 0.75:\n    print(f\"  ⚠️  Precision OK ({precision.numpy()*100:.2f}%)\")\n    score_list.append(\"ok\")\nelse:\n    print(f\"  ❌ Precision LOW ({precision.numpy()*100:.2f}%)\")\n    score_list.append(\"low\")\n\n# Final recommendation\nprint(f\"\\n\" + \"=\"*60)\nif \"excellent\" in score_list and recall.numpy() > 0.85:\n    print(\"🎉 READY FOR PRODUCTION!\")\n    print(\"✅ Model performs excellently\")\n    print(\"✅ Can be deployed to GUI\")\nelif \"good\" in score_list and recall.numpy() > 0.80:\n    print(\"✅ READY FOR DEPLOYMENT\")\n    print(\"✅ Model performs well\")\n    print(\"✅ Acceptable for production\")\nelse:\n    print(\"⚠️  NEEDS IMPROVEMENT\")\n    print(\"⚠️  Consider retraining\")\n    print(\"⚠️  Adjust hyperparameters\")\n\nprint(\"=\"*60)\n\n# =====================================================\n# SAVE RESULTS TO FILE\n# =====================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SAVING RESULTS\")\nprint(\"=\"*60)\n\n# Create results dictionary\nresults = {\n    'precision': float(precision.numpy()),\n    'recall': float(recall.numpy()),\n    'specificity': float(specificity.numpy()),\n    'f1': float(f1.numpy()),\n    'accuracy': float(accuracy.numpy()),\n    'dice': float(dice),\n    'norm_accuracy': float(norm_acc),\n    'tp': int(tp.numpy()),\n    'fp': int(fp.numpy()),\n    'tn': int(tn.numpy()),\n    'fn': int(fn.numpy()),\n    'test_samples': test_size\n}\n\n# Save to JSON\nimport json\nwith open('final_model_evaluation.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"✓ Results saved to: final_model_evaluation.json\")\n\n# =====================================================\n# INFERENCE FUNCTION\n# =====================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"READY FOR INFERENCE\")\nprint(\"=\"*60)\n\ndef predict_final_model(ecg_512):\n    \"\"\"\n    Predict SVE mask for 512-sample ECG window\n    \n    Args:\n        ecg_512: numpy array of shape (512,) or (512, 1)\n    \n    Returns:\n        Probability mask (512,) with values 0-1\n    \"\"\"\n    if ecg_512.ndim == 1:\n        ecg_512 = ecg_512.reshape(-1, 1)\n    X = ecg_512.reshape(1, 512, 1)\n    return final_model.predict(X, verbose=0).squeeze()\n\nprint(\"\\nUsage examples:\")\nprint(\"  # Get probability prediction\")\nprint(\"  pred = predict_final_model(X_test[0])\")\nprint(\"\\n  # Get binary mask\")\nprint(\"  mask = (pred > 0.5).astype(int)\")\nprint(\"\\n  # Get SVE burden\")\nprint(\"  burden = np.mean(mask) * 100\")\nprint(\"\\n✓ Ready to use!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:42:48.678050Z","iopub.status.idle":"2025-12-14T08:42:48.678282Z","shell.execute_reply.started":"2025-12-14T08:42:48.678178Z","shell.execute_reply":"2025-12-14T08:42:48.678188Z"}},"outputs":[],"execution_count":null}]}